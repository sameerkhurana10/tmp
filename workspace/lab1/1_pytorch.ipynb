{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your name: \n",
    "your-name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigned reading:\n",
    "Chapters 1-3 of textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: MNIST: PyTorch\n",
    "\n",
    "To get you started we have provided a \"deep\" network in this notebook. Run the ipython\n",
    "notebook as you go along and answer the questions. Most questions below have a *small* coding component (you only need to edit code when specifically asked to)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time, os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "%matplotlib inline\n",
    "\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor(),\n",
    "     torchvision.transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "split = 10000\n",
    "\n",
    "# shuffle data\n",
    "np.random.seed(6825)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=50, sampler=train_sampler, shuffle=False)\n",
    "\n",
    "validloader = torch.utils.data.DataLoader(trainset, batch_size=50, sampler=valid_sampler, shuffle=False)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the network\n",
    "\n",
    "Network description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, 5, padding = 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 5, padding = 2)\n",
    "        self.fc1 = nn.Linear(8 * 7 * 7, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 8 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Analysis\n",
    "### Question 1.\n",
    "\n",
    "Read the code above and determine the type of each of the layers (convolutional, fully-connected, ReLU, or pooling) and then answer the following questions by updating the cell below:\n",
    "\n",
    "1. How many layers deep is the network and what is the type of each layer (count nonlinear activations as their own layer, but do not count flattening)?\n",
    " \n",
    "2. What is the size of the network input? Your answer should be a 4-D tuple `(batch_size, width, height, channels)`\n",
    "3. Describe the parameters used in each layer:\n",
    "    1. Convolutional layers: Specify a 4-tuple `(weight_width, weight_height, channels, filter_count)`\n",
    "    2. Fully connected layers: Specify a 2-tuple `(num_output_nodes, num_input_nodes)`\n",
    "    3. Pool layer: Specify `(x_window, y_window, x_stride, y_stride)`\n",
    "    4. ReLU: Specify `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update\n",
    "num_layers = 4\n",
    "layer_type = ['fc', 'pool', 'conv', 'relu']\n",
    "network_input_sz = (0, 0, 0, 0)\n",
    "\n",
    "layer_param = [\n",
    "    (0, 0),\n",
    "    (0, 0, 0, 0),\n",
    "    (0, 0, 0, 0),\n",
    "    None\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.\n",
    "\n",
    "One way of finding the layer input sizes is simply by inspection. Since the inputs of a subsequent layer are the outputs of a previous layer, we can also compute the size of these outputs based on the inputs sizes and weight parameters. Complete the `get_output_size` function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_size(input_sz, layer_type, layer_param):\n",
    "    if layer_type == 'conv':\n",
    "        # Your code here   \n",
    "        # Return format: (batch_size, width, height, channels)\n",
    "        return (0, 0, 0, 0)\n",
    "    elif layer_type == 'pool':\n",
    "        # Your code here\n",
    "        # Return format: (batch_size, width, height, channels)\n",
    "        return (0, 0, 0, 0)\n",
    "    elif layer_type == 'fc':\n",
    "        # Your code here\n",
    "        # Return format: (batch_size, num_outputs)\n",
    "        return (0, 0)\n",
    "    elif layer_type == 'relu':\n",
    "        # Your code here\n",
    "        # Return format: Input-dependent tuple \n",
    "        return None\n",
    "\n",
    "layer_sz = []\n",
    "print(\"Input  : \", network_input_sz)\n",
    "for n in range(num_layers):\n",
    "    if n == 0:\n",
    "        layer_sz.append(get_output_size(network_input_sz, layer_type[n], layer_param[n]))\n",
    "    else:\n",
    "        layer_sz.append(get_output_size(layer_sz[n-1], layer_type[n], layer_param[n]))\n",
    "    print(\"Layer %d: \" % (n+1), layer_sz[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.\n",
    "\n",
    "Next, complete the `num_params` and `param_memory_size` functions to calculate the number of weights (i.e. parameters, including biases) required in each layer and the memory required for storing the weights (including biases) respectively. Assume weight is stored in single precision floating point format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_params(layer_type, layer_param):\n",
    "    if layer_type == 'conv':\n",
    "        # Your code here\n",
    "        # Return format: (number_of_params)\n",
    "        return 0\n",
    "    elif layer_type == 'pool':\n",
    "        # Your code here\n",
    "        # Return format: (number_of_params)\n",
    "        return 0\n",
    "    elif layer_type == 'fc':\n",
    "        # Your code here\n",
    "        # Return format: (number_of_params)\n",
    "        return 0\n",
    "    elif layer_type == 'relu':\n",
    "        # Your code here\n",
    "        # Return format: (number_of_params)\n",
    "        return 0\n",
    "\n",
    "# Required memory in bytes\n",
    "def param_memory_size(layer_type, layer_param):\n",
    "    # Your code here\n",
    "    # Return format: (mem_size_for_params)\n",
    "    return 0\n",
    "\n",
    "layer_params_mem = []\n",
    "for n in range(num_layers):\n",
    "    layer_params_mem.append(param_memory_size(layer_type[n], layer_param[n]))\n",
    "    print(\"Layer %d: \" % (n+1), layer_params_mem[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.\n",
    "\n",
    "Determine the number of multiplications required per _batch_. Multiplications by zero should still be counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_mult(input_sz, layer_type, layer_param):\n",
    "    if layer_type == 'conv':\n",
    "        # Your code here\n",
    "        # Return format: (number_of_mult)\n",
    "        return 0\n",
    "    elif layer_type == 'pool':\n",
    "        # Your code here\n",
    "        # Return format: (number_of_mult)\n",
    "        return 0\n",
    "    elif layer_type == 'fc':\n",
    "        # Your code here\n",
    "        # Return format: (number_of_mult)\n",
    "        return 0\n",
    "    elif layer_type == 'relu':\n",
    "        # Your code here\n",
    "        # Return format: (number_of_mult)\n",
    "        return 0\n",
    "\n",
    "layer_mult_count = []\n",
    "for n in range(num_layers):\n",
    "    if n == 0:\n",
    "        layer_mult_count.append(num_mult(network_input_sz, layer_type[n], layer_param[n]))\n",
    "    else:\n",
    "        layer_mult_count.append(num_mult(layer_sz[n-1], layer_type[n], layer_param[n]))\n",
    "    print(\"Layer %d: \" % (n+1), layer_mult_count[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Summary\n",
    "\n",
    "Run this cell to summarize your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Network Summary:\")\n",
    "print(\"Layer\\tType\\tInput Size\\tWeight Param\\tOutput Size\\tWeight Memory\\t#mult\")\n",
    "for layer_idx in range(num_layers):\n",
    "    print(\"%d\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\" % (\n",
    "            (layer_idx+1),\n",
    "            layer_type[layer_idx], \n",
    "            str(network_input_sz if layer_idx == 0 else layer_sz[layer_idx-1]).ljust(12), \n",
    "            str(layer_param[layer_idx]).ljust(12), \n",
    "            str(layer_sz[layer_idx]).ljust(12), \n",
    "            str(layer_params_mem[layer_idx]).ljust(12),\n",
    "            str(layer_mult_count[layer_idx]).ljust(12)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below trains for `num_epochs` number of epochs and plots the training error. If you want (not graded), you can update this function to calculate the validation accuracy after each epoch so that you can plot it later. You can then play around with the number of epochs to see how it affects the validation accuracy (also not graded).\n",
    "\n",
    "Note: The initialization below is not strictly necessary, as PyTorch will automoatically initialize the weights (including biases) for you. We've included initialization here so that if you run the cell more than once, you will start fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_acc_vect = np.zeros(num_epochs)\n",
    "valid_acc_vect = np.zeros(num_epochs)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# initialize weights and biases\n",
    "nn.init.kaiming_uniform_(net.conv1.weight, nonlinearity = 'relu')\n",
    "stdv = 1./np.sqrt(net.conv1.bias.size(0))\n",
    "nn.init.uniform_(net.conv1.bias, -stdv, stdv)\n",
    "nn.init.kaiming_uniform_(net.conv2.weight, nonlinearity = 'relu')\n",
    "stdv = 1./np.sqrt(net.conv2.bias.size(0))\n",
    "nn.init.uniform_(net.conv2.bias, -stdv, stdv)\n",
    "nn.init.kaiming_uniform_(net.fc1.weight, nonlinearity = 'relu')\n",
    "stdv = 1./np.sqrt(net.fc1.bias.size(0))\n",
    "nn.init.uniform_(net.fc1.bias, -stdv, stdv)\n",
    "nn.init.kaiming_uniform_(net.fc2.weight, nonlinearity = 'relu')\n",
    "stdv = 1./np.sqrt(net.fc2.bias.size(0))\n",
    "nn.init.uniform_(net.fc2.bias, -stdv, stdv)\n",
    "\n",
    "# train network\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times while training\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data # list of [inputs, labels]\n",
    "        #print(labels.shape)\n",
    "        optimizer.zero_grad() # clear gradients\n",
    "        outputs = net(inputs) # forward step\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward() # backprop\n",
    "        optimizer.step() # optimize weights\n",
    "\n",
    "        # print statistics\n",
    "        duration = time.time() - start_time\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    \n",
    "    training_acc = correct_train / total_train * 100\n",
    "    training_acc_vect[epoch] = training_acc\n",
    "    \n",
    "    print('Accuracy of the network on the 50000 training images after epoch %d: %.2f %% (%.1f sec)' % (\n",
    "        epoch + 1, training_acc, duration))\n",
    "    \n",
    "    # your code here to calculate the validation error after each epoch\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_vect = np.linspace(1, num_epochs, num_epochs)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(epoch_vect, 100-training_acc_vect)\n",
    "\n",
    "print(\"Final Training Accuracy: %g\" % (training_acc))\n",
    "\n",
    "# Your code here to plot validation error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.\n",
    "\n",
    "(Theory question, no coding component.) You have trained a classifier to recognize handwritten digits with a training set of black digits on a white background. You then give it test images with white digits on a black background, however, it doesn't seem to perform the classification correctly. Please list the principles that could explain why it doesn't work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-your explanation here-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting and loading the model, performing inference\n",
    "\n",
    "The following 2 lines save the model to the location specified by PATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './my_mnist_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.\n",
    "\n",
    "Run inference *using the saved model* and print the training, validation and test accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_net = Net()\n",
    "\n",
    "def eval_model(PATH, trainloader, validloader, testloader):\n",
    "\n",
    "    # your code here\n",
    "    training_accuracy = 0\n",
    "    validation_accuracy = 0\n",
    "    test_accuracy = 0\n",
    "        \n",
    "    return training_accuracy, validation_accuracy, test_accuracy\n",
    "\n",
    "training_accuracy, validation_accuracy, test_accuracy = eval_model(PATH, trainloader, validloader, testloader)\n",
    "print('Training Accuracy: %g' % training_accuracy)        \n",
    "print('Validation Accuracy: %g' % validation_accuracy)\n",
    "print('Test Accuracy: %g' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7.\n",
    "\n",
    "Edit the code below to save the weights and biases for the convolutional and fully-connected layers only. They should be saved as numpy arrays into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = {}\n",
    "\n",
    "# Your code here\n",
    "# format (not necessarily as a for loop):\n",
    "# for i in range(num_layers):\n",
    "#     model_parameters['l' + str(i) + '_w'] = np.array([]) # _w for weights or _b for biases\n",
    "\n",
    "np.save('my_model_parameters.npy', model_parameters, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8.\n",
    "We will evaluate the inference-time energy and latency of the neural network we trained above on the model of a custom acclerator design. The profiler uses the [timeloop/accelergy](http://accelergy.mit.edu/tutorial.html) commands, which we will cover more in-depth in the later part of this course, on the convolution/fully-connected layers in the neural network to obtain the energy estimates. This part may take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelergyTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profiler import Profiler\n",
    "profiler = Profiler(\n",
    "    top_dir='workloads',\n",
    "    sub_dir='mnist',\n",
    "    timeloop_dir='simple_weight_stationary',\n",
    "    model=net,\n",
    "    input_size=(1, 28, 28),\n",
    "    batch_size=1,\n",
    "    convert_fc=True,\n",
    "    exception_module_names=[]\n",
    ")\n",
    "\n",
    "results = profiler.profile()\n",
    "\n",
    "total_energy = 0\n",
    "total_cycle = 0\n",
    "\n",
    "for layer_id, info in results.items():\n",
    "    print(f\"ID: {layer_id} \\t Energy: {info['energy']} \\t Cycle: {info['cycle']} \\t Number of same architecture layers: {info['num']}\")\n",
    "    total_energy += info['energy'] * info['num']\n",
    "    total_cycle += info['cycle'] * info['num']\n",
    "    \n",
    "print(f'\\nTotal Energy: {total_energy} pj \\nTotal Cycles: {total_cycle}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the total energy and cycles below:\n",
    "\n",
    "Energy:\n",
    "\n",
    "\n",
    "-your answers here-\n",
    "\n",
    "\n",
    "\n",
    "Cycles:\n",
    "\n",
    "\n",
    "-your answers here-\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
