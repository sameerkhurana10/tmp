{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your name: \n",
    "Sameer Khurana (skhurana)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigned reading:\n",
    "Chapters 1-3 of textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: MNIST: PyTorch\n",
    "\n",
    "To get you started we have provided a \"deep\" network in this notebook. Run the ipython\n",
    "notebook as you go along and answer the questions. Most questions below have a *small* coding component (you only need to edit code when specifically asked to)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time, os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "%matplotlib inline\n",
    "\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor(),\n",
    "     torchvision.transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "split = 10000\n",
    "\n",
    "# shuffle data\n",
    "np.random.seed(6825)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=50, sampler=train_sampler, shuffle=False)\n",
    "\n",
    "validloader = torch.utils.data.DataLoader(trainset, batch_size=50, sampler=valid_sampler, shuffle=False)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# DELETE THIS\n",
    "\n",
    "for ex in trainloader:\n",
    "    print(ex[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the network\n",
    "\n",
    "Network description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, 5, padding = 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 5, padding = 2)\n",
    "        self.fc1 = nn.Linear(8 * 7 * 7, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 8 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Analysis\n",
    "### Question 1.\n",
    "\n",
    "Read the code above and determine the type of each of the layers (convolutional, fully-connected, ReLU, or pooling) and then answer the following questions by updating the cell below:\n",
    "\n",
    "1. How many layers deep is the network and what is the type of each layer (count nonlinear activations as their own layer, but do not count flattening)?\n",
    " \n",
    "2. What is the size of the network input? Your answer should be a 4-D tuple `(batch_size, width, height, channels)`\n",
    "3. Describe the parameters used in each layer:\n",
    "    1. Convolutional layers: Specify a 4-tuple `(weight_width, weight_height, channels, filter_count)`\n",
    "    2. Fully connected layers: Specify a 2-tuple `(num_output_nodes, num_input_nodes)`\n",
    "    3. Pool layer: Specify `(x_window, y_window, x_stride, y_stride)`\n",
    "    4. ReLU: Specify `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Types:  ['conv', 'relu', 'pool', 'conv', 'relu', 'pool', 'fc', 'relu', 'fc']\n",
      "Layer Param:  [(5, 5, 1, 4), None, (2, 2, 2, 2), (5, 5, 4, 8), None, (2, 2, 2, 2), (256, 392), None, (10, 256)]\n"
     ]
    }
   ],
   "source": [
    "# update\n",
    "num_layers = 9\n",
    "\n",
    "layer_type = ['conv', 'relu', 'pool', 'conv', 'relu', 'pool', 'fc', 'relu', 'fc']\n",
    "\n",
    "network_input_sz = (50, 28, 28, 1)\n",
    "\n",
    "conv1_param = (5, 5, 1, 4)\n",
    "\n",
    "conv2_param = (5, 5, 4, 8)\n",
    "\n",
    "fc1_param = (256, 392)\n",
    "\n",
    "fc2_param = (10, 256)\n",
    "\n",
    "pool_layer = (2, 2, 2, 2)\n",
    "\n",
    "relu = None\n",
    "\n",
    "layer_param = [\n",
    "    conv1_param,\n",
    "    None,\n",
    "    pool_layer,\n",
    "    conv2_param,\n",
    "    None,\n",
    "    pool_layer,\n",
    "    fc1_param,\n",
    "    None,\n",
    "    fc2_param,\n",
    "]\n",
    "print(\"Layer Types: \", layer_type)\n",
    "print(\"Layer Param: \", layer_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.\n",
    "\n",
    "One way of finding the layer input sizes is simply by inspection. Since the inputs of a subsequent layer are the outputs of a previous layer, we can also compute the size of these outputs based on the inputs sizes and weight parameters. Complete the `get_output_size` function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input  :  (50, 28, 28, 1)\n",
      "Layer 1:  (50, 28, 28, 4)\n",
      "Layer 2:  (50, 28, 28, 4)\n",
      "Layer 3:  (50, 14, 14, 4)\n",
      "Layer 4:  (50, 14, 14, 8)\n",
      "Layer 5:  (50, 14, 14, 8)\n",
      "Layer 6:  (50, 7, 7, 8)\n",
      "Layer 7:  (50, 256)\n",
      "Layer 8:  (50, 256)\n",
      "Layer 9:  (50, 10)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def get_output_size(input_sz, layer_type, layer_param):\n",
    "    \n",
    "    # Input shape: (N, W, H, C)\n",
    "    \n",
    "    batch_size = input_sz[0]\n",
    "    \n",
    "    if len(input_sz) == 4:\n",
    "\n",
    "        width_in = input_sz[1]\n",
    "\n",
    "        height_in = input_sz[2]\n",
    "\n",
    "        channels = input_sz[3]\n",
    "\n",
    "    if layer_type == 'conv':\n",
    "        # Your code here\n",
    "        weight_width, weight_height, channels, filter_count = layer_param\n",
    "        \n",
    "        padding = 2.\n",
    "        \n",
    "        stride = 1.\n",
    "        \n",
    "        height = 1.+((height_in + 2*padding - (weight_height-1)-1) / stride)\n",
    "        height = math.floor(height)\n",
    "        \n",
    "        width = 1.+((width_in + 2*padding - (weight_width-1)-1) / stride)\n",
    "        width = math.floor(width)\n",
    "        \n",
    "        # Return format: (batch_size, width, height, channels)\n",
    "        return (batch_size, width, height, filter_count)\n",
    "    \n",
    "    elif layer_type == 'pool':\n",
    "        \n",
    "        # Your code here\n",
    "        \n",
    "        padding = 0\n",
    "        \n",
    "        (x_window, y_window, x_stride, y_stride) = layer_param\n",
    "        \n",
    "        height = 1.+((height_in + 2*padding - (y_window-1)-1) / y_stride)\n",
    "        height = math.floor(height)\n",
    "        \n",
    "        width = 1.+((width_in + 2*padding - (x_window-1)-1) / x_stride)\n",
    "        width = math.floor(width)\n",
    "        \n",
    "        # Return format: (batch_size, width, height, channels)\n",
    "        \n",
    "        return (batch_size, width, height, channels)\n",
    "    \n",
    "    elif layer_type == 'fc':\n",
    "        \n",
    "        # Your code here\n",
    "        \n",
    "        (num_output_nodes, num_input_nodes) = layer_param\n",
    "        \n",
    "        # Return format: (batch_size, num_outputs)\n",
    "        \n",
    "        return (batch_size, num_output_nodes)\n",
    "    \n",
    "    elif layer_type == 'relu':\n",
    "        \n",
    "        # Your code here\n",
    "        \n",
    "        # Return format: Input-dependent tuple \n",
    "        \n",
    "        return input_sz\n",
    "\n",
    "layer_sz = []\n",
    "print(\"Input  : \", network_input_sz)\n",
    "for n in range(num_layers):\n",
    "    if n == 0:\n",
    "        layer_sz.append(get_output_size(network_input_sz, layer_type[n], layer_param[n]))\n",
    "    else:\n",
    "        layer_sz.append(get_output_size(layer_sz[n-1], layer_type[n], layer_param[n]))\n",
    "    print(\"Layer %d: \" % (n+1), layer_sz[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight 100\n",
      "conv1.bias 4\n",
      "conv2.weight 800\n",
      "conv2.bias 8\n",
      "fc1.weight 100352\n",
      "fc1.bias 256\n",
      "fc2.weight 2560\n",
      "fc2.bias 10\n"
     ]
    }
   ],
   "source": [
    "for p in net.named_parameters():\n",
    "    params = np.prod(p[1].size())\n",
    "    print(p[0], params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.\n",
    "\n",
    "Next, complete the `num_params` and `param_memory_size` functions to calculate the number of weights (i.e. parameters, including biases) required in each layer and the memory required for storing the weights (including biases) respectively. Assume weight is stored in single precision floating point format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1:  416.0\n",
      "Layer 2:  0.0\n",
      "Layer 3:  0.0\n",
      "Layer 4:  3232.0\n",
      "Layer 5:  0.0\n",
      "Layer 6:  0.0\n",
      "Layer 7:  402432.0\n",
      "Layer 8:  0.0\n",
      "Layer 9:  10280.0\n"
     ]
    }
   ],
   "source": [
    "def num_params(layer_type, layer_param):\n",
    "    if layer_type == 'conv':\n",
    "        \n",
    "        # Your code here\n",
    "        \n",
    "        weight_width, weight_height, channels, filter_count = layer_param\n",
    "        \n",
    "        # +1 for bias\n",
    "        total_params = ((weight_width * weight_height * channels)+1)*filter_count\n",
    "        \n",
    "        # Return format: (number_of_params)\n",
    "        \n",
    "        return total_params\n",
    "    \n",
    "    elif layer_type == 'pool':\n",
    "        \n",
    "        # Your code here\n",
    "        \n",
    "        total_params = 0\n",
    "        \n",
    "        # Return format: (number_of_params)\n",
    "        \n",
    "        return total_params\n",
    "    \n",
    "    elif layer_type == 'fc':\n",
    "        \n",
    "        # Your code here\n",
    "        \n",
    "        (num_output_nodes, num_input_nodes) = layer_param\n",
    "        \n",
    "        # +num_output_nodes for bias\n",
    "        total_params = (num_output_nodes * num_input_nodes) + num_output_nodes\n",
    "        \n",
    "        # Return format: (number_of_params)\n",
    "        \n",
    "        return total_params\n",
    "    \n",
    "    elif layer_type == 'relu':\n",
    "        \n",
    "        # Your code here\n",
    "        \n",
    "        total_params = 0\n",
    "        \n",
    "        # Return format: (number_of_params)\n",
    "        \n",
    "        return total_params\n",
    "\n",
    "# Required memory in bytes\n",
    "def param_memory_size(layer_type, layer_param):\n",
    "    \n",
    "    # Your code here\n",
    "    \n",
    "    params = num_params(layer_type, layer_param)\n",
    "    \n",
    "    bit_storage = params * 32.\n",
    "    \n",
    "    mem = bit_storage * 0.125\n",
    "    \n",
    "    # Return format: (mem_size_for_params)\n",
    "    \n",
    "    return mem\n",
    "\n",
    "layer_params_mem = []\n",
    "for n in range(num_layers):\n",
    "    layer_params_mem.append(param_memory_size(layer_type[n], layer_param[n]))\n",
    "    print(\"Layer %d: \" % (n+1), layer_params_mem[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.\n",
    "\n",
    "Determine the number of multiplications required per _batch_. Multiplications by zero should still be counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1:  3920000\n",
      "Layer 2:  0\n",
      "Layer 3:  0\n",
      "Layer 4:  7840000\n",
      "Layer 5:  0\n",
      "Layer 6:  0\n",
      "Layer 7:  5017600\n",
      "Layer 8:  0\n",
      "Layer 9:  128000\n"
     ]
    }
   ],
   "source": [
    "def num_mult(input_sz, layer_type, layer_param):\n",
    "    if layer_type == 'conv':\n",
    "        # Your code here\n",
    "        \n",
    "        weight_width, weight_height, channels_in, filter_count = layer_param\n",
    "        \n",
    "        (batch_size, width_out, height_out, channels_out) = get_output_size(input_sz, layer_type, layer_param)\n",
    "        \n",
    "        num_mult = (weight_width * weight_height * channels_in) * width_out * height_out * channels_out * batch_size\n",
    "        \n",
    "        # Return format: (number_of_mult)\n",
    "        \n",
    "        return num_mult\n",
    "    elif layer_type == 'pool':\n",
    "        \n",
    "        # Your code here\n",
    "        \n",
    "        # Return format: (number_of_mult)\n",
    "        \n",
    "        return 0\n",
    "    elif layer_type == 'fc':\n",
    "        \n",
    "        # Your code here\n",
    "        \n",
    "        batch_size = input_sz[0]\n",
    "        \n",
    "        (num_output_nodes, num_input_nodes) = layer_param\n",
    "        \n",
    "        num_mult = batch_size * num_output_nodes * num_input_nodes\n",
    "        \n",
    "        # Return format: (number_of_mult)\n",
    "        \n",
    "        return num_mult\n",
    "    elif layer_type == 'relu':\n",
    "        \n",
    "        # Your code here\n",
    "        \n",
    "        \n",
    "        # Return format: (number_of_mult)\n",
    "        \n",
    "        return 0\n",
    "\n",
    "layer_mult_count = []\n",
    "for n in range(num_layers):\n",
    "    if n == 0:\n",
    "        layer_mult_count.append(num_mult(network_input_sz, layer_type[n], layer_param[n]))\n",
    "    else:\n",
    "        layer_mult_count.append(num_mult(layer_sz[n-1], layer_type[n], layer_param[n]))\n",
    "    print(\"Layer %d: \" % (n+1), layer_mult_count[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Summary\n",
    "\n",
    "Run this cell to summarize your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Summary:\n",
      "Layer\tType\tInput Size\tWeight Param\tOutput Size\tWeight Memory\t#mult\n",
      "1\tconv\t(50, 28, 28, 1)\t(5, 5, 1, 4)\t(50, 28, 28, 4)\t416.0       \t3920000     \n",
      "2\trelu\t(50, 28, 28, 4)\tNone        \t(50, 28, 28, 4)\t0.0         \t0           \n",
      "3\tpool\t(50, 28, 28, 4)\t(2, 2, 2, 2)\t(50, 14, 14, 4)\t0.0         \t0           \n",
      "4\tconv\t(50, 14, 14, 4)\t(5, 5, 4, 8)\t(50, 14, 14, 8)\t3232.0      \t7840000     \n",
      "5\trelu\t(50, 14, 14, 8)\tNone        \t(50, 14, 14, 8)\t0.0         \t0           \n",
      "6\tpool\t(50, 14, 14, 8)\t(2, 2, 2, 2)\t(50, 7, 7, 8)\t0.0         \t0           \n",
      "7\tfc\t(50, 7, 7, 8)\t(256, 392)  \t(50, 256)   \t402432.0    \t5017600     \n",
      "8\trelu\t(50, 256)   \tNone        \t(50, 256)   \t0.0         \t0           \n",
      "9\tfc\t(50, 256)   \t(10, 256)   \t(50, 10)    \t10280.0     \t128000      \n"
     ]
    }
   ],
   "source": [
    "print(\"Network Summary:\")\n",
    "print(\"Layer\\tType\\tInput Size\\tWeight Param\\tOutput Size\\tWeight Memory\\t#mult\")\n",
    "for layer_idx in range(num_layers):\n",
    "    print(\"%d\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\" % (\n",
    "            (layer_idx+1),\n",
    "            layer_type[layer_idx], \n",
    "            str(network_input_sz if layer_idx == 0 else layer_sz[layer_idx-1]).ljust(12), \n",
    "            str(layer_param[layer_idx]).ljust(12), \n",
    "            str(layer_sz[layer_idx]).ljust(12), \n",
    "            str(layer_params_mem[layer_idx]).ljust(12),\n",
    "            str(layer_mult_count[layer_idx]).ljust(12)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below trains for `num_epochs` number of epochs and plots the training error. If you want (not graded), you can update this function to calculate the validation accuracy after each epoch so that you can plot it later. You can then play around with the number of epochs to see how it affects the validation accuracy (also not graded).\n",
    "\n",
    "Note: The initialization below is not strictly necessary, as PyTorch will automoatically initialize the weights (including biases) for you. We've included initialization here so that if you run the cell more than once, you will start fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 50000 training images after epoch 1: 80.66 % (626.3 sec)\n",
      "Accuracy of the network on the validation images after epoch 1: 91.96 % (626.3 sec)\n",
      "Accuracy of the network on the 50000 training images after epoch 2: 92.94 % (1470.0 sec)\n",
      "Accuracy of the network on the validation images after epoch 2: 94.25 % (1470.0 sec)\n",
      "Accuracy of the network on the 50000 training images after epoch 3: 94.74 % (2338.3 sec)\n",
      "Accuracy of the network on the validation images after epoch 3: 95.62 % (2338.3 sec)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_966/2388339197.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtotal_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;31m# list of [inputs, labels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#print(labels.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/bin/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/bin/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/bin/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/bin/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/bin/conda/lib/python3.8/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/bin/conda/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/bin/conda/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/bin/conda/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;31m# put it from HWC to CHW format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_acc_vect = np.zeros(num_epochs)\n",
    "valid_acc_vect = np.zeros(num_epochs)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# initialize weights and biases\n",
    "nn.init.kaiming_uniform_(net.conv1.weight, nonlinearity = 'relu')\n",
    "stdv = 1./np.sqrt(net.conv1.bias.size(0))\n",
    "nn.init.uniform_(net.conv1.bias, -stdv, stdv)\n",
    "nn.init.kaiming_uniform_(net.conv2.weight, nonlinearity = 'relu')\n",
    "stdv = 1./np.sqrt(net.conv2.bias.size(0))\n",
    "nn.init.uniform_(net.conv2.bias, -stdv, stdv)\n",
    "nn.init.kaiming_uniform_(net.fc1.weight, nonlinearity = 'relu')\n",
    "stdv = 1./np.sqrt(net.fc1.bias.size(0))\n",
    "nn.init.uniform_(net.fc1.bias, -stdv, stdv)\n",
    "nn.init.kaiming_uniform_(net.fc2.weight, nonlinearity = 'relu')\n",
    "stdv = 1./np.sqrt(net.fc2.bias.size(0))\n",
    "nn.init.uniform_(net.fc2.bias, -stdv, stdv)\n",
    "\n",
    "# train network\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times while training\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data # list of [inputs, labels]\n",
    "        #print(labels.shape)\n",
    "        optimizer.zero_grad() # clear gradients\n",
    "        outputs = net(inputs) # forward step\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward() # backprop\n",
    "        optimizer.step() # optimize weights\n",
    "\n",
    "        # print statistics\n",
    "        duration = time.time() - start_time\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    \n",
    "    training_acc = correct_train / total_train * 100\n",
    "    training_acc_vect[epoch] = training_acc\n",
    "    \n",
    "    print('Accuracy of the network on the 50000 training images after epoch %d: %.2f %% (%.1f sec)' % (\n",
    "        epoch + 1, training_acc, duration))\n",
    "    \n",
    "    # your code here to calculate the validation error after each epoch\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_acc = correct / total * 100\n",
    "    print('Accuracy of the network on the validation images after epoch %d: %.2f %% (%.1f sec)' % (\n",
    "        epoch + 1, val_acc, duration))\n",
    "    \n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Accuracy: 94.738\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_vect = np.linspace(1, 3, 3)\n",
    "\n",
    "plt.figure(1)\n",
    "train_line, = plt.plot(epoch_vect, 100-training_acc_vect[:3])\n",
    "\n",
    "print(\"Final Training Accuracy: %g\" % (training_acc))\n",
    "\n",
    "# Your code here to plot validation error\n",
    "val_accs = np.array([91.96, 94.25, 95.62])\n",
    "valid_line, = plt.plot(epoch_vect, 100-val_accs)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error[%]')\n",
    "plt.legend([train_line, valid_line], [\"Train\", \"Valid\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.\n",
    "\n",
    "(Theory question, no coding component.) You have trained a classifier to recognize handwritten digits with a training set of black digits on a white background. You then give it test images with white digits on a black background, however, it doesn't seem to perform the classification correctly. Please list the principles that could explain why it doesn't work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is the problem of domain mismatch between the training and testing conditions. During training the model used black pixel values to detect the digit. In the process the model must learn to ignore a lot of white pixels and concentrate only on black pixels to predict the digit. Hence, during testing the model does not do well with white digits on black pixels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting and loading the model, performing inference\n",
    "\n",
    "The following 2 lines save the model to the location specified by PATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './my_mnist_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.\n",
    "\n",
    "Run inference *using the saved model* and print the training, validation and test accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [03:42<00:00,  4.50it/s]\n",
      "100%|| 200/200 [00:43<00:00,  4.56it/s]\n",
      "100%|| 200/200 [00:43<00:00,  4.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 96.092\n",
      "Validation Accuracy: 95.2\n",
      "Test Accuracy: 96.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "loaded_net = Net()\n",
    "\n",
    "def eval_model(PATH, trainloader, validloader, testloader):\n",
    "    \n",
    "    state_dict = torch.load(PATH)\n",
    "    loaded_net.load_state_dict(state_dict)\n",
    "    \n",
    "    # your code here\n",
    "    training_accuracy = 0\n",
    "    validation_accuracy = 0\n",
    "    test_accuracy = 0\n",
    "    \n",
    "    def get_accuracy(data_loader):\n",
    "        acc = 0\n",
    "        tot = 0\n",
    "        corr = 0\n",
    "        for i, data in enumerate(tqdm.tqdm(data_loader, total=len(data_loader))):\n",
    "            inputs, labels = data\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            tot += labels.size(0)\n",
    "            corr += (predicted == labels).sum().item()\n",
    "        \n",
    "        acc = corr / tot * 100\n",
    "        return acc\n",
    "    \n",
    "    training_accuracy = get_accuracy(trainloader)\n",
    "    validation_accuracy = get_accuracy(validloader)\n",
    "    test_accuracy = get_accuracy(testloader)\n",
    "        \n",
    "    return training_accuracy, validation_accuracy, test_accuracy\n",
    "\n",
    "training_accuracy, validation_accuracy, test_accuracy = eval_model(PATH, trainloader, validloader, testloader)\n",
    "print('Training Accuracy: %g' % training_accuracy)        \n",
    "print('Validation Accuracy: %g' % validation_accuracy)\n",
    "print('Test Accuracy: %g' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7.\n",
    "\n",
    "Edit the code below to save the weights and biases for the convolutional and fully-connected layers only. They should be saved as numpy arrays into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = {}\n",
    "\n",
    "# Your code here\n",
    "\n",
    "layer_ids = [i for i, x in enumerate(layer_type) if x == 'conv' or x == 'fc']\n",
    "\n",
    "idx = 0\n",
    "for layer in net.children():\n",
    "    if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "        weight = layer.weight\n",
    "        bias = layer.bias\n",
    "        layer_id = layer_ids[idx] \n",
    "        model_parameters['l' + str(layer_id) + '_w'] = weight.detach().numpy()\n",
    "        model_parameters['l' + str(layer_id) + '_b'] = bias.detach().numpy()\n",
    "        idx += 1\n",
    "\n",
    "np.save('my_model_parameters.npy', model_parameters, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8.\n",
    "We will evaluate the inference-time energy and latency of the neural network we trained above on the model of a custom acclerator design. The profiler uses the [timeloop/accelergy](http://accelergy.mit.edu/tutorial.html) commands, which we will cover more in-depth in the later part of this course, on the convolution/fully-connected layers in the neural network to obtain the energy estimates. This part may take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelergyTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting nn.Conv2d and nn.Linear in mnist model ...\n",
      "workload file --> /home/workspace/lab1/workloads/mnist/mnist_layer1.yaml\n",
      "workload file --> /home/workspace/lab1/workloads/mnist/mnist_layer2.yaml\n",
      "workload file --> /home/workspace/lab1/workloads/mnist/mnist_layer3.yaml\n",
      "workload file --> /home/workspace/lab1/workloads/mnist/mnist_layer4.yaml\n",
      "conversion complete!\n",
      "\n",
      "running timeloop to get energy and latency...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [07:08<00:00, 107.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeloop running finished!\n",
      "ID: 1 \t Energy: 1683439.4 \t Cycle: 19600 \t Number of same architecture layers: 1\n",
      "ID: 2 \t Energy: 2368815.15 \t Cycle: 4900 \t Number of same architecture layers: 1\n",
      "ID: 3 \t Energy: 14177701.54 \t Cycle: 448 \t Number of same architecture layers: 1\n",
      "ID: 4 \t Energy: 390710.34 \t Cycle: 16 \t Number of same architecture layers: 1\n",
      "\n",
      "Total Energy: 18620666.43 pj \n",
      "Total Cycles: 24964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from profiler import Profiler\n",
    "profiler = Profiler(\n",
    "    top_dir='workloads',\n",
    "    sub_dir='mnist',\n",
    "    timeloop_dir='simple_weight_stationary',\n",
    "    model=net,\n",
    "    input_size=(1, 28, 28),\n",
    "    batch_size=1,\n",
    "    convert_fc=True,\n",
    "    exception_module_names=[]\n",
    ")\n",
    "\n",
    "results = profiler.profile()\n",
    "\n",
    "total_energy = 0\n",
    "total_cycle = 0\n",
    "\n",
    "for layer_id, info in results.items():\n",
    "    print(f\"ID: {layer_id} \\t Energy: {info['energy']} \\t Cycle: {info['cycle']} \\t Number of same architecture layers: {info['num']}\")\n",
    "    total_energy += info['energy'] * info['num']\n",
    "    total_cycle += info['cycle'] * info['num']\n",
    "    \n",
    "print(f'\\nTotal Energy: {total_energy} pj \\nTotal Cycles: {total_cycle}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the total energy and cycles below:\n",
    "\n",
    "Energy:\n",
    "\n",
    "\n",
    "18620666.43 pj\n",
    "\n",
    "\n",
    "\n",
    "Cycles:\n",
    "\n",
    "24964\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
