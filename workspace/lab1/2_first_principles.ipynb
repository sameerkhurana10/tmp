{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your name:\n",
    "Sameer Khurana (skhurana)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: MNIST: First Principles\n",
    "\n",
    "In this notebook, we will recreate the functions that PyTorch provides for the individual layers in our network using primtive Python code. The goal of this notebook is to understand what happens in a network underneath the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import MNIST test data as numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# download test set using torchvision\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor(),\n",
    "     torchvision.transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1)\n",
    "\n",
    "# convert to numpy array\n",
    "images_array = torch.zeros((10000,28,28))\n",
    "labels_array = torch.zeros(10000)\n",
    "for i, data in enumerate(testloader, 0):\n",
    "    image, label = data\n",
    "    images_array[i,:,:] = image\n",
    "    labels_array[i] = label\n",
    "images_array = images_array.numpy()\n",
    "labels_array = labels_array.numpy()\n",
    "labels_array = labels_array.astype(int)\n",
    "print(images_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the trained model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9.\n",
    "\n",
    "Visualize the first test image and label.\n",
    "\n",
    "Load the model parameters (the file you saved in pytorch.ipynb). Similarly visualize the weights of the filters used in the first convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make sure you have saved model_parameters.npy\n",
    "model_parameters = np.load('my_model_parameters.npy', allow_pickle=True)\n",
    "model_parameters = model_parameters[()]\n",
    "\n",
    "plt.figure(1)\n",
    "# Your code to display one input image here (with its label)\n",
    "plt.imshow(-1*images_array[0], cmap='gray')\n",
    "plt.text(10, -2, \"Label is \" + str(labels_array[0]))\n",
    "plt.axis('off')\n",
    "l1_filter = model_parameters['l0_w'] # update\n",
    "num_input_channels = l1_filter.shape[1]\n",
    "num_out_channels = l1_filter.shape[0] # filters\n",
    "plt.figure(2)\n",
    "for x in range(num_out_channels): # filters\n",
    "    for y in range(num_input_channels): # channels\n",
    "        plt.subplot(num_input_channels, num_out_channels, y*num_out_channels + x + 1)\n",
    "        # Your code for the filter here\n",
    "        plt.imshow(-1*l1_filter[x, y], cmap=\"gray\")\n",
    "        plt.title(\"In: %d, Out: %d\" % (y, x))\n",
    "        plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the network\n",
    "\n",
    "### Question 10.\n",
    "\n",
    "We need to write code to run the individual layers in our network. For simiplicity, assume that the batch size is 1, i.e., the activation layers are numpy arrays with shape (channels, height, width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(x, F, b, U=1, padding=2):\n",
    "    C, H, W = x.shape\n",
    "    M, C, R, S = F.shape\n",
    "    P = (H - R + 2*padding + U) / U\n",
    "    Q = (W - S + 2*padding + U) / U\n",
    "    P = math.floor(P)\n",
    "    Q = math.floor(Q)\n",
    "    x = np.pad(x, pad_width=((0, 0), (2, 2), (2, 2)))\n",
    "    O = np.zeros((M, P, Q))\n",
    "    for m in range(M):\n",
    "        for p in range(P):\n",
    "            for q in range(Q):\n",
    "                for c in range(C):\n",
    "                    for r in range(R):\n",
    "                        for s in range(S):\n",
    "                            O[m][p][q] += (x[c][U*p+r][U*q+s] * F[m][c][r][s])\n",
    "    \n",
    "    return O + b[:, None, None]\n",
    "\n",
    "def fc(x, W, b):\n",
    "    out = W@x + b\n",
    "    return out\n",
    "\n",
    "def relu(x):\n",
    "    out = np.maximum(x, 0)\n",
    "    return out\n",
    "\n",
    "def pool2(x, dh, dw, U=2, padding=0):\n",
    "    C, H, W = x.shape\n",
    "    P = (H - dh + 2*padding + U) / U\n",
    "    Q = (W - dw + 2*padding + U) / U\n",
    "    P = math.floor(P)\n",
    "    Q = math.floor(Q)\n",
    "    O = np.zeros((C, P, Q))\n",
    "    for p in range(P):\n",
    "        for q in range(Q):\n",
    "            for c in range(C):\n",
    "                for r in range(dh):\n",
    "                    for s in range(dw):\n",
    "                        val = x[c][U*p+r][U*q+s]\n",
    "                        if O[c, p, q] < val:\n",
    "                            O[c, p, q] = val\n",
    "    return O\n",
    "\n",
    "def flatten(x):\n",
    "    # Your code here\n",
    "    return x.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11.\n",
    "\n",
    "Compose the layers from above to create the network from `mnist/pytorch.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(image, model_parameters):\n",
    "    # Your code for defining the correct network topology here, using the saved model parameters\n",
    "    # replace code below\n",
    "    conv1_w = model_parameters['l0_w']\n",
    "    conv1_b = model_parameters['l0_b']\n",
    "    conv2_w = model_parameters['l3_w']\n",
    "    conv2_b = model_parameters['l3_b']\n",
    "    fc1_w = model_parameters['l6_w']\n",
    "    fc1_b = model_parameters['l6_b']\n",
    "    fc2_w = model_parameters['l8_w']\n",
    "    fc2_b = model_parameters['l8_b']\n",
    "    x = conv(image, conv1_w, conv1_b)\n",
    "    x = relu(x)\n",
    "    x = pool2(x, 2, 2)\n",
    "    x = conv(x, conv2_w, conv2_b)\n",
    "    x = relu(x)\n",
    "    x = pool2(x, 2, 2)\n",
    "    x = x.flatten()\n",
    "    x = fc(x, fc1_w, fc1_b)\n",
    "    x = relu(x)\n",
    "    x = fc(x, fc2_w, fc2_b)\n",
    "    output_class = np.argmax(x) # calculate output label here\n",
    "    return output_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to verify your network on the first 1000 images from the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [47:15<00:00,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 950/1000 (0.95)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct, total = (0, 1000)\n",
    "for n in tqdm(range(min(total, int(images_array.shape[0])))):\n",
    "    inference = run_inference(images_array[n][None, ...], model_parameters)\n",
    "    if labels_array[n] == inference:\n",
    "        correct += 1\n",
    "print(\"Accuracy: %d/%d (%.2g)\" % (correct, total, float(correct)/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12.\n",
    "You have built a programmable system that performs classification on MNIST, where the input image size is 28x28 @ 95% accuracy. It only spends 0.1nJ per classification and operates at 100 classifications per second. You then use it to perform classification on ImageNet, where the input image size is 224x224 @ 95% accuracy. However, you noticed that the energy and latency scale much more than 224x224/(28x28). What could be causing the non-linear scaling? Answer in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-your-answer-here-\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
