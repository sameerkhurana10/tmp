{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your name:\n",
    "-your-name-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: ImageNet: AlexNet\n",
    "\n",
    "This notebook implements the AlexNet network for the 1000-class classification task (Task 1) from the Large Scale Visual Recognition Challenge 2012 ([ILSVRC2012](http://www.image-net.org/challenges/LSVRC/2012/)). This implementation is adapted from [here](https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic imports and setups\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import urllib\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the network\n",
    "\n",
    "This model was inspired by AlexNet (though it doesn't include local response normalization as in the original 2012 paper). Reference: https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "\n",
    "__all__ = ['AlexNet', 'alexnet']\n",
    "\n",
    "model_url = 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth'\n",
    "\n",
    "# we define the network here\n",
    "class AlexNet_custom(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet_custom, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6)) # in case input image is larger\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# we load pretrained weights\n",
    "def alexnet_custom(pretrained=True, progress=True, **kwargs):\n",
    "    '''AlexNet model architecture from the\n",
    "    \"One weird trick...\" <https://arxiv.org/abs/1404.5997> paper.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    '''\n",
    "    model = AlexNet_custom(**kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_url, progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "net = alexnet_custom()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sample test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we download example images\n",
    "\n",
    "urls = [\"https://s3.amazonaws.com/mlpipes/pytorch-quick-start/cat.jpg\",\\\n",
    "        \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\",\\\n",
    "        \"https://upload.wikimedia.org/wikipedia/commons/c/c2/Hawaiian_Sea_Lobster.JPG\"]\n",
    "filenames = [\"cat.jpg\",\"dog.jpg\",\"Hawaiian_Sea_Lobster.JPG\"]\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# we load images into torch tensors, then display them (where pyplot requires the images to be transposed)\n",
    "input_tensors = torch.zeros((len(urls),3,224,224))\n",
    "fig = plt.figure(figsize=(20,6))\n",
    "for i in range(len(urls)):\n",
    "    url, filename = (urls[i], filenames[i])\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    input_image = Image.open(filename)\n",
    "\n",
    "    input_tensor = preprocess(input_image)\n",
    "    input_tensors[i,:,:,:] = input_tensor\n",
    "\n",
    "    img = input_tensor.numpy()\n",
    "    img = img - np.min(img)\n",
    "    img = img/np.max(img)\n",
    "    img = img.transpose((1,2,0))\n",
    "    fig.add_subplot(1,len(urls),i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify the sample test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    out = net(input_tensors)\n",
    "# Tensor of shape 1000, with confidence scores over ImageNet's 1000 classes\n",
    "# The output has unnormalized scores. To get probabilities, we run a softmax on it.\n",
    "\n",
    "with open('imagenet_classes_init.txt') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "_, index = torch.max(out, 1)\n",
    "\n",
    "percentage = torch.nn.functional.softmax(out, dim=1)[1] * 100\n",
    "\n",
    "fig = plt.figure(figsize=(20,6))\n",
    "# Loop over all images\n",
    "for i in range(len(urls)):\n",
    "    # Plot image with class name and prob in the title\n",
    "    percentage = torch.nn.functional.softmax(out, dim=1)[i] * 100\n",
    "    \n",
    "    img = input_tensors[i].numpy()\n",
    "    img = img - np.min(img)\n",
    "    img = img/np.max(img)\n",
    "    img = img.transpose((1,2,0))\n",
    "    fig.add_subplot(1,len(urls),i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Class: \" + classes[index[i]] + \"\\nprobability: %.4f%%\" % percentage[index[i]].item())\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13.\n",
    "\n",
    "Repeat the calculations from questions 1-4 for __ONLY__ the convolutional, pool and fully-connected layers in the modified AlexNet implemented above.\n",
    "\n",
    "fc:\n",
    "`(num_output_nodes, num_input_nodes)`\n",
    "\n",
    "pool:\n",
    "`(x_window, y_window, x_stride, y_stride)`\n",
    "\n",
    "conv:\n",
    "`(width, height, channels, filter_count, stride_x, stride_y, padding)`\n",
    "\n",
    "(Note that the convolutional layer parameters are a bit different now.)\n",
    "\n",
    "The fully-connected layers are very similar to the ones you have already seen. This time, *do not* include ReLU or dropout in the number of layers or layer type list. You can also ignore AdaptiveAvgPool2d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit these variables to the correct values\n",
    "\n",
    "num_layers = 2\n",
    "layer_type = ['conv', 'fc']\n",
    "\n",
    "layer_input_sz = [\n",
    "    (0,0,0,0),\n",
    "    (0,0)\n",
    "]\n",
    "\n",
    "layer_param = [\n",
    "    (0,0,0,0,0,0,0),\n",
    "    (0,0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_size(inputs_sz, layer_type, layer_param):\n",
    "    if layer_type == 'conv':\n",
    "        # Return format: (batch_size, width, height, channels)\n",
    "        # Your code here\n",
    "        return (0, 0, 0, 0)\n",
    "    elif layer_type == 'pool':\n",
    "        # Return format: (batch_size, width, height, channels)\n",
    "        # Your code here\n",
    "        return (0, 0, 0, 0)\n",
    "    elif layer_type == 'fc':\n",
    "        # Return format: (batch_size, num_outputs)\n",
    "        # Your code here\n",
    "        return (0, 0)\n",
    "\n",
    "def num_params(layer_type, layer_param):\n",
    "    if layer_type == 'conv':\n",
    "        # Return format: (number_of_params)\n",
    "        # Your code here\n",
    "        return 0\n",
    "    elif layer_type == 'pool':\n",
    "        # Return format: (number_of_params)\n",
    "        # Your code here\n",
    "        return 0\n",
    "    elif layer_type == 'fc':\n",
    "        # Return format: (number_of_params)\n",
    "        # Your code here\n",
    "        return 0\n",
    "\n",
    "# Required memory in bytes\n",
    "def param_memory_size(layer_type, layer_param):    \n",
    "    # Return format: (mem_size_for_params)\n",
    "    # Your code here\n",
    "    return 0\n",
    "\n",
    "def num_mult(input_sz, layer_type, layer_param):\n",
    "    if layer_type == 'conv':\n",
    "        # Return format: (number_of_mult)\n",
    "        # Your code here\n",
    "        return 0\n",
    "    elif layer_type == 'pool':\n",
    "        # Return format: (number_of_mult)\n",
    "        # Your code here\n",
    "        return 0\n",
    "    elif layer_type == 'fc':\n",
    "        # Return format: (number_of_mult)\n",
    "        # Your code here\n",
    "        return 0\n",
    "\n",
    "layer_output_sz = []\n",
    "layer_params_mem = []\n",
    "layer_mult_count = []\n",
    "\n",
    "for n in range(num_layers):\n",
    "    layer_output_sz.append(get_output_size(layer_input_sz[n], layer_type[n], layer_param[n]))\n",
    "    layer_params_mem.append(param_memory_size(layer_type[n], layer_param[n]))\n",
    "    layer_mult_count.append(num_mult(layer_input_sz[n], layer_type[n], layer_param[n]))\n",
    "\n",
    "print(\"Network Summary:\")\n",
    "print(\"# Type\\tInput Size\\t\\tWeight Param\\t\\t\\t\\tOutput Size\\t\\tWeight Memory\\t#mult\")\n",
    "for layer_idx in range(num_layers):\n",
    "    print(\"%d %s\\t%s\\t%s\\t%s\\t%s\\t%s\" % (\n",
    "            (layer_idx+1),\n",
    "            layer_type[layer_idx], \n",
    "            str(layer_input_sz[layer_idx]).ljust(16), \n",
    "            str(layer_param[layer_idx]).ljust(32), \n",
    "            str(layer_output_sz[layer_idx]).ljust(16), \n",
    "            str(layer_params_mem[layer_idx]).ljust(12),\n",
    "            str(layer_mult_count[layer_idx]).ljust(12)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14.\n",
    "\n",
    "Count the number of zero input activations to the convolutional and fully-connected layers for the `dog.jpg` image. Be careful about how you index the layers in this step (check the network definition above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(net, input_tensors):\n",
    "    act = []\n",
    "    # your code here to get a list of input activations of the convolutional and fully-connected layers\n",
    "\n",
    "    return act\n",
    "\n",
    "act = get_activations(net, input_tensors[1].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Layer\\tTotal\\tZeros\\tZero Percentage\")\n",
    "nonzero_sum = 0\n",
    "tot_sum = 0\n",
    "for i in range(len(act)):\n",
    "    act_i = act[i].numpy()\n",
    "    tot, zeros = act_i.size, np.count_nonzero(act_i==0)\n",
    "    nonzero_sum += tot-zeros\n",
    "    tot_sum += tot\n",
    "    frac = 100.0*zeros/tot\n",
    "    print(\"%d\\t%d\\t%d\\t%g\" % (i+1, tot, zeros, frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question 15.\n",
    "\n",
    "Now, we would like to determine the impact of the zero activations on the number of multiplications. Assuming you do not have to perform multiplications for zero activations, count the number of non-zero multiplications when running inference for the `dog.jpg` image.\n",
    "\n",
    "__Simplifying Assumptions:__\n",
    "- You can assume that all the learned layer parameters, i.e. weights, are non-zero\n",
    "- You can ignore the impact of padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "total_mult = 1\n",
    "non_zero_mult = 0\n",
    "\n",
    "frac = 100.0*non_zero_mult/total_mult\n",
    "# ----------\n",
    "\n",
    "print(\"Total Mul\\tNon-Zero Mul\\tNon-Zero Percentage\")\n",
    "print(\"%d\\t%d\\t%g\" % (total_mult, non_zero_mult, frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 16.\n",
    "\n",
    "Similar to the neural network we used for mnist, we will evaluate the inference energy and latency for the alexnet model we examined here. Run the below codes, and report the total energy and cycles required to compute convolutional layers in the alexnet. This part may take more than 1 hour to finish, grab a cup of coffee and check back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelergyTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from profiler import Profiler\n",
    "profiler = Profiler(\n",
    "    top_dir='workloads',\n",
    "    sub_dir='alexnet',\n",
    "    timeloop_dir='simple_weight_stationary',\n",
    "    model=net,\n",
    "    input_size=(3, 224, 224),\n",
    "    batch_size=1,\n",
    "    convert_fc=True,\n",
    "    exception_module_names=[]\n",
    ")\n",
    "\n",
    "results = profiler.profile()\n",
    "\n",
    "total_energy = 0\n",
    "total_cycle = 0\n",
    "\n",
    "for layer_id, info in results.items():\n",
    "    print(f\"ID: {layer_id} \\t Energy: {info['energy']} \\t Cycle: {info['cycle']} \\t Number of same architecture layers: {info['num']}\")\n",
    "    total_energy += info['energy'] * info['num']\n",
    "    total_cycle += info['cycle'] * info['num']\n",
    "    \n",
    "print(f'\\nTotal Energy: {total_energy} pj \\nTotal Cycles: {total_cycle}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the total energy and cycles below:\n",
    "\n",
    "Energy:\n",
    "\n",
    "\n",
    "-your answers here-\n",
    "\n",
    "\n",
    "\n",
    "Cycles:\n",
    "\n",
    "\n",
    "-your answers here-\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
